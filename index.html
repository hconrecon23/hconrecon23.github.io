<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>EASM 🤿</title>

		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">
		<link rel="stylesheet" href="dist/custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>EASM - A deep 🤿 </h1>
					<h4>https://hconrecon23.github.io</h4>
				</section>
				<section>
					<h1>WHOAMI</h1>
					<h3>SIX2DEZ</h3>
					<img src="profile_pic.jpg" style="border-radius:50%;width:20%;"/>
					<ul>
						<li style="font-size: 0.7em;">Padre, ethical hacker y bash lover en <img style="vertical-align: middle;width: 4em;margin: 0px;" data-src="visma_logo.png">  Red Team</li>
						<li style="font-size: 0.7em;">Pentester en Cobalt Core y bug hunter</li>
						<li style="font-size: 0.7em;">Principales proyectos FOSS: <a href="https://github.com/six2dez/reconftw">reconFTW</a>, <a href="https://github.com/six2dez/OneListForAll">OneListForAll</a>, <a href="https://pentestbook.six2dez.com/">pentest-book</a></li>
						<li style="font-size: 0.7em;">Escalada, senderismo y cualquier cosa relacionada con la montaña</li>
						<li style="font-size: 0.7em;"><a href="https://twitter.com/six2dez1">@six2dez1</a> en Twitter, <a href="https://github.com/six2dez">@six2dez</a> en el resto</li>
					</ul>
				</section>
				<section>
					<h1>WHOAMI</h1>
					<h3>joohoi</h3>
          			<img src="joohoi_profile.jpg" style="border-radius:50%;width: 20%;" /><br />
					<ul>
						<li>Team lead <img style="vertical-align: middle;width: 4em;margin: 0px;" data-src="visma_logo.png"> red team</li>
						<li>Proyectos open source: <strong>acme-dns</strong>, <strong>ffuf</strong>, <strong>certbot</strong></li>
						<li>Padre, geek de los juegos de mesa, cervecero</li>
						<li><a href="https://twitter.com/joohoi">@joohoi</a> en todos las RRSS</li>
					</ul>
        		</section>
				<section>
					<h2>Eligiendo el objetivo</h2>
					<section>
						<p style="font-size: 0.7em;">(a.k.a. correlación horizontal)</p>
						<img src="https://media.giphy.com/media/3zDdFSPALuCe6C43nM/giphy.gif" width="60%" height="auto"/>
					</section>
					<section>
						<h4>
							Información básica 1/2
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Vistazo inicial a la web corporativa
								<ul>
									<li style="font-size: 0.8em;">Sobre nosotros</li>
									<li style="font-size: 0.8em;">Contacto</li>
								</ul>
							</li>
							<li style="font-size: 0.7em;"><a href="https://www.exploit-db.com/google-hacking-database">Google</a>/<a href="https://github.com/techgaun/github-dorks/blob/master/github-dorks.txt">Github</a> Dorks</li>
							<li style="font-size: 0.7em;">Emails, usuarios, tecnologías
								<ul>
									<li style="font-size: 0.8em;">Metadatos en documentos indexados</li>
									<li style="font-size: 0.8em;">Cuentas en redes sociales</li>
								</ul>
							</li>
							<li style="font-size: 0.7em;">Brechas de datos</li>
						</ul>
						<aside class="notes">
							metafinder<br>
							emailfinder<br>
							https://breachdirectory.org/ + https://hashes.com/en/decrypt/hash
						</aside>
					</section>
					<section>
						<h4>
							Información básica 2/2
						</h4>
						<pre data-id="code-animation"><code>
			# Metadata finder
			metafinder -d "target.tld" -l 10 -go -bi -ba -o path
			
			# Email finder
			emailfinder -d "target.tld"
			
			# Users/Employees
			python3 crosslinked.py -f '{first}.{last}@domain.com' company
			site:linkedin.com/in "domain.com" # Dork

			# Data breaches for free
			https://breachdirectory.org/
			https://hashes.com/en/decrypt/hash</code></pre>
					</section>
					<section>
						<br>
						<br>
						<h4>
							<a href="https://www.crunchbase.com/">Crunchbase</a>
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Fusiones y adquisiciones</li>
							<li style="font-size: 0.7em;">Dominios asociados</li>
							<li style="font-size: 0.7em;">Información histórica</li>
							<li style="font-size: 0.7em;">Otros: tech stack, supply chain</li>
						</ul>
						<p>
							<img src="https://media.giphy.com/media/l1LcbR6oWTJkJllOE/giphy.gif" width="60%" height="auto"/>
						</p>
					</section>
					<section>
						<br>
						<h4>
							ASN recon
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Detectar ASN dedicados</li>
							<li style="font-size: 0.7em;">Expandir bloques de IPs</li>
							<li style="font-size: 0.7em;">Proveedor de hosting</li>
							<li style="font-size: 0.7em;">Mapear estructura de red</li>
						</ul>
						<pre data-id="code-animation"><code>
	# Manual
	bgp.he.net, ipinfo.io, asnlookup.com
	
	# Fetch info from ASN
	asnmap -i 1.3.3.7 -org GOOGLE -d facebook.com -a AS3941
	amass intel -org "COMPANY"
						</code></pre>
					</section>
					<section>
						<h4>
							Registrants / reverse whois
						</h4>
						<p>Owner name, company name, email o palabras clave, es una relación que requiere investigar recursivamente debido a que las empresas usan distintos emails y nombres para registrar sus dominios.</p>
						<p>Difícil de automatizar o recuperar sin interacción desde cli. Altas probabilidades de salirse de scope.</p>
						<ul>
							<li style="font-size: 0.7em;"><a href="https://viewdns.info/">Viewdns</a></li>
							<li style="font-size: 0.7em;"><a href="https://www.whoxy.com/">Whoxy</a></li>
							<li style="font-size: 0.7em;"><a href="https://iqwhois.com/">iqwhois</a></li>
							<li style="font-size: 0.7em;"><a href="https://www.whoisxmlapi.com/">WhoisXMLAPI</a></li>
						</ul>
					</section>
					<section>
						<h4>
							Favicon Hash
						</h4>
						<p>Útil para descubrir dominios relacionados, requiere el <a href="https://github.com/edoardottt/favirecon">cálculo</a> del hash del <a href="https://cyberwarzone.com/online-favicon-checksum-tool/#">favicon</a> y tener cuenta en Shodan, ojo a rediseños o logotipos antiguos 😜</p>
						<pre data-id="code-animation"><code>
				# Shodan query
				http.favicon.hash:FAVICONHASH

				# Tool
				python3 favUp.py -wl web_list.txt -sc
						</code></pre>
					</section>
					<section>
						<h4>
							Recursividad
						</h4>
						<img src="https://media.giphy.com/media/agSmzrhrXNiWA/giphy.gif" width="25%" height="auto"/>
					</section>
				</section>
				<section>
					<h2>Descubrimiento de activos</h2>
					<section>
						<p>
							<img src="https://media.giphy.com/media/YlAuPqtjwlSyzy8PZU/giphy-downsized-large.gif" width="auto" height="auto"/>
						</p>
					</section>
					<section>
						<h4>
							Enumeración pasiva de subdominios 1/4
						</h4>
						<p>Forma más fácil y rápida de recuperar subdominios, info desactualizada, modelo de gasto de créditos/tokens por consulta en la mayoría de servicios.</p>
						<ul>
							<li style="font-size: 0.7em;">Datos de servicios de terceros
								<ul>
									<li style="font-size: 0.8em;">Fuentes (<a href="https://securitytrails.com/">SecurityTrails</a>, <a href="https://censys.io/">Censys</a>, ... <a href="https://docs.google.com/spreadsheets/d/1ssuiovzgvFH2aTK-ymrxy2VezvHw5fnvXxBMH3Jnok4/edit?usp=sharing">+200</a>)</li>
									<li style="font-size: 0.8em;">Tools (<a href="https://github.com/OWASP/Amass">Amass</a>, <a href="https://github.com/projectdiscovery/subfinder">Subfinder</a>, <a href="https://github.com/tomnomnom/assetfinder">Assetfinder</a>)</li>
								</ul>
							</li>
							<li style="font-size: 0.7em;"><a href="https://crt.sh/">Transparencia de certificados</a></li>
							<li style="font-size: 0.7em;">Código en proveedores de control de versions (GitHub, GitLab, etc.)</li>
						</ul>
					</section>
					<section>
						<h4>
							Enumeración pasiva de subdominios 2/4
						</h4>
						<pre data-id="code-animation"><code>
					# Amass passive data fetch
					amass enum -passive -d target.tl	
					subfinder -d target.tl
					
					# Ctfr subdomain search on crtsh
					python3 ctfr.py -d target.tl
					
					# Subdomains from Github
					github-subdomains -d target.tld -t tokens.txt

					# Subdomains from GitLab
					gitlab-subdomains -d target.tld
						</code></pre>
					</section>
					<section>
						<h4>
							Enumeración pasiva de subdominios 3/4
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Registros DNS</li>
							<li style="font-size: 0.7em;"><a href="https://builtwith.com/">Relaciones</a><a href="https://osint.sh/analytics/"> cruzadas</a> <a href="https://site-overview.com/">Analytics</a>
								<ul>
									<li style="font-size: 0.8em;">Google Analytics</li>
									<li style="font-size: 0.8em;">Google Tag Manager</li>
									<li style="font-size: 0.8em;">Facebook Pixel Tag</li>
									<li style="font-size: 0.8em;">NewRelic</li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<h4>
							Enumeración pasiva de subdominios 4/4
						</h4>
						<pre data-id="code-animation"><code>
				# Fetching DNS records with dnsx
				cat subs.txt | dnsx -cname -ns -mx -soa -resp					
				
				# Analytics from BuiltWith and HackerTarget
				cat file.txt | analyticsrelationships 					
				
				# Analytics from SpyOnWeb, site-overview.com
				# 	osint.sh and HackerTarget
				udon -silent -json -s UA-33427076 | jq -c
						</code></pre>
					</section>
					<section>
						<h4>Enumeración activa de subdominios 1/3</h4>
						<p>Alto coste en tiempo y recursos, pero resultados únicos y valiosos</p>
						<p>Requisitos</p>
							<ul>
								<li style="font-size: 0.7em;"><a href="https://github.com/trickest/resolvers/blob/main/resolvers.txt">Resolvers?</a></li>
								<li style="font-size: 0.7em;"><a href="https://github.com/six2dez/resolvers_reconftw/blob/main/resolvers_trusted.txt">Trusted resolvers???</a></li>
								<li style="font-size: 0.7em;">Resolución DNS en masa (a.k.a. <a href="https://github.com/blechschmidt/massdns">massdns</a>)</li>
								<li style="font-size: 0.7em;">Wordlists, genéricas o específicas</li>
							</ul>
					</section>
					<section>
						<br>
						<br>
						<h4>Enumeración activa de subdominios 2/3</h4>
							<ul>
								<li style="font-size: 0.7em;">Fuerza bruta con <a href="https://gist.github.com/six2dez/0e07fd3461338374cc3ef942e1e9ddaf">wordlists</a> genéricas</li>
								<li style="font-size: 0.7em;"><a href="https://gist.github.com/six2dez/942b01af47204958045e7c392c250c86">Permutaciones</a>
									<ul>
										<li style="font-size: 0.8em;">Generación de wordlists</li>
										<li style="font-size: 0.8em;">Análisis de reglas regex</li>
										<li style="font-size: 0.8em;">IA???</li>
									</ul>
								</li>
							</ul>
							<pre data-id="code-animation"><code>
		# Gotator wordlist generation
		gotator -sub subdomains.txt -perm permutations.txt \
		-depth 1 -numbers 5 -mindup -adv -md > permutations.txt

		# Regulator
		python3 main.py -t target.tld -f subdomains.txt -o output.rules
							</code></pre>
					</section>
					<section>
						<br>
						<h4>
							Enumeración activa de subdominios 3/3
						</h4>
						<pre data-id="code-animation"><code>
	# Resolvers generation
	dnsvalidator -tL https://public-dns.info/nameservers.txt \
	-thread 200 > resolvers.txt
	
	# DNS resolution
	puredns bruteforce wordlist.txt target.com \
	-r resolvers.txt --resolvers-trusted res_trusted.txt \
	-l 1000 --wildcard-tests 10 -w output.txt

	# Amass
	amass enum -df doms.txt -w subs.txt -aw perms.txt \
	-rf resolvers.txt -trf res_trust.txt -trqps 400
						</code></pre>
						<p style="font-size: 0.7em;">⚠️ Puedes acabar DOSeando el objetivo</p>
					</section>
					<section>
						<h4>
							Other techniques
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Análisis de certificados x509</li>
							<li style="font-size: 0.7em;">DNS <a href="https://www.securesystems.de/blog/enhancing-subdomain-enumeration-ents-and-noerror/">NOERROR</a> (subdominios anidados)</li>
						</ul>
						<pre data-id="code-animation"><code>
			# x509 subdomain extraction
			cat subs.txt | tlsx -san -cn -silent -r											
			
			# DNS NOERROR
			echo "random.target.tld" | dnsx -rcode noerror,nxdomain
			dnsx -d target.tld -rcode noerror -w wordlist.txt
							</code></pre>
					</section>
					<section>
						<br>
						<br>
						<h4>
							Recursividad
						</h4>
						<img src="https://media.giphy.com/media/agSmzrhrXNiWA/giphy.gif" width="auto" height="auto"/>
					</section>
				</section>
				<section>
					<h2>Escaneo de puertos & descubrimiento de servicios</h2>
					<section>
						<br>
						<br>
						<br>
						<p>
							<img src="https://media.giphy.com/media/xCwYFe19SldXLrJlwm/giphy.gif" width="auto" height="auto"/>
						</p>
					</section>
					<section>
						<br>
						<br>
						<h4>
							Buscar hosts
						</h4>
						<p>Es esencial resolver las IP para evitar resoluciones DNS innecesarias al escanear puertos y/o escanear el mismo host más de una vez</p>
						<ul>
							<li style="font-size: 0.7em;">Resolución DNS de las etapas anteriores</li>
							<li style="font-size: 0.7em;">Obtener IPs a partir de CIDRs o ASN</li>
						</ul>
						<pre data-id="code-animation"><code>
				# Get A/AAAA records from subdomains
				dnsx -a -aaaa -resp -silent -l subdomains.txt
				
				# Expand CIDRs/ASN
				echo "ASD_OR_CIDR" | mapcidr -silent
						</code></pre>
					</section>
					<section>
						<br>
						<h4>
							Detección de servicios en la nube
						</h4>
						<p style="font-size: 0.7em;"><a href="https://cdn.nuclei.sh/">Rangos de IPs en la nube, WAFs, CDNs</a></p>
						<p>El objetivo es evitar escaneos de puertos innecesarios y evitar baneos o bloqueos.</p>
						<pre data-id="code-animation"><code>
								# WAF and CDN detection
								cat list.txt | ipcdn -v

								# Cloud detection by bruteforce
								python3 cloud_enum.py -k keyword
						</code></pre>
					</section>
					<section>
						<h4>
							Escaneo de puertos pasivo y activo
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Pasivo: Shodan cli/smap
								<ul>
									<li style="font-size: 0.8em;">Rápido</li>
									<li style="font-size: 0.8em;">Sin interacción con el target</li>
									<li style="font-size: 0.8em;">Desactualizado</li>
								</ul>
							</li>
							<li style="font-size: 0.7em;">Activo: Masscan/nmap
								<ul>
									<li style="font-size: 0.8em;">Depende de las opciones, pero más lento en general</li>
									<li style="font-size: 0.8em;">Más ruidoso ya que requiere interacción con el target</li>
									<li style="font-size: 0.8em;">Datos en tiempo real</li>
								</ul>
							</li>
						</ul>
					</section>
					<section>
						<br>
						<h4>
							Ejemplos de escaneos de puertos
						</h4>
						<pre data-id="code-animation"><code>
						# Passive ports scan
						smap -iL targets.txt
						shodan host 10.X

						# Active ports scan
						masscan 10.X.X.X -p1-1000
						nmap -iL targets.txt
						nmap -iL targets.txt -sV --script vulners 
							--script-args mincvss=7.0
						</code></pre>

					</section>
					<section>
						<br>
						<h4>
							Expandir pasos anteriores
						</h4>
						<p style="font-size: 0.7em;">Reverse DNS records / PTR</p>
						<p style="font-size: 0.7em;">⚠️ Peligro de salirse de scope (cloud, hosting compartido, etc.)</p>
						<pre data-id="code-animation"><code>
				# PTR records
				cat iplist.txt | dnsx -ptr -retry 3 -silent -resp
						</code></pre>
					</section>
				</section>
				<section>
					<h2>Inspección de las tecnologías</h2>
					<section>
						<br>
						<br>
						<br>
						<p>
							<img src="https://media.giphy.com/media/1uyFaGpt2ilmE/giphy-downsized-large.gif" width="auto" height="auto"/>
						</p>
					</section>
					<section>
						<h4>
							Versiones de software
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Útil para etapas posteriores
								<ul>
									<li style="font-size: 0.8em;">Ataques específicos</li>
									<li style="font-size: 0.8em;">Vulnerabilidades conocidas</li>
								</ul>
							</li>
							<li style="font-size: 0.7em;">Ayuda para priorizar</li>
							<li style="font-size: 0.7em;">Nivel de actualizaciones</li>
						</ul>
					</section>
					<section>
						<h4>
							Banner grabbing / servicios custom
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Igual que las versiones de software</li>
							<li style="font-size: 0.7em;">OS Fingerprinting</li>
							<li style="font-size: 0.7em;">Servicios custom o desconocidos == echar un ojo</li>
						</ul>
					</section>
					<section>
						<br>
						<br>
						<br>
						<h4>
							CMS, frameworks, WAF
						</h4>
						<p style="font-size: 0.7em;">Historial de vulnerabilidades conocidas en diferentes CMS (WordPress, Drupal), Frameworks (Symfony, Laravel) y bypasses según el WAF</p>
						<pre data-id="code-animation"><code>
			# Web tech detection
			httpx -title -server -tech-detect -cdn -l list.txt
			
			# CMS detection
			python3 cmseek.py -l tartget_list.txt
			
			# WAF detection
			wafw00f -l tartget_list.txt
						</code></pre>
					</section>
					<section>
						<br>
						<br>
						<h4>
							Inpección visual / web screenshooting
						</h4>
						<p style="font-size: 0.7em;">Ayuda a filtrar miles de capturas de pantalla y la mayoría de las herramientas proporcionan informes</p>
						<p style="font-size: 0.7em;">Podemos aprovecharnos de la IA para clasificar los sitios según su aspecto</p>
						<pre data-id="code-animation"><code>
			# Web screenshoting
			gowitness file -f sites.tx			

			# Classify based on given pretrained AI model
			python3 eyeballer.py --weights model.h5 evaluate /path/
						</code></pre>
					</section>
				</section>
				<section>
					<h2>Low hanging fruits</h2>
					<section>
						<br>
						<br>
						<p>
							<img src="https://media.giphy.com/media/0r51Zmo72WICswWagi/giphy.gif" width="auto" height="auto"/>
						</p>
					</section>
					<section>
						<h4>
							CVE's and vulnerabilidades conocidas
						</h4>
						<ul>
							<li style="font-size: 0.7em;">CVE Trends</li>
							<li style="font-size: 0.7em;">Credenciales por defecto</li>
							<li style="font-size: 0.7em;">Vulnerabilidades típicas de una máquina Easy de HTB</li>
						</ul>
					</section>
					<section>
						<br>
						<br>
						<h4>
							Nuclei
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Reemplazo FOSS para soluciones comerciales de escáneo de vulnerabilidades</li>
							<li style="font-size: 0.7em;">Basado en plantillas</li>
							<li style="font-size: 0.7em;">Plantillas diseñadas por la comunidad</li>
							<li style="font-size: 0.7em;">Desde detección de software hasta explotación de vulns críticas</li>
							<li style="font-size: 0.7em;">Facilidad para escribir tus propias plantillas</li>
						</ul>
						<pre data-id="code-animation"><code>
						# Fetch latest templates
						nuclei -ut
						
						# Scan horizontally for specific CVE
						cat targets.txt | nuclei -id CVE-2023-1337
						</code></pre>
					</section>
					<section>
						<h4>
							Archivos comunes con info sensible
						</h4>
						<ul>
							<li style="font-size: 0.7em;">Revelación de credenciales</li>
							<li style="font-size: 0.7em;">Información interna confidencial</li>
							<li style="font-size: 0.7em;">Información crítica sobre otros activos</li>
							<li style="font-size: 0.7em;"><a href="https://github.com/six2dez/OneListForAll/blob/main/onelistforallmicro.txt">Archivos interesantes</a>: .conf,.env,.sftp_config,etc</li>
							<li style="font-size: 0.7em;">PII</li>
						</ul>
					</section>
					<section>
						<br>
						<h4>
							Subdomain takeovers
						</h4>
						<p>Registros DNS mal configurados que apuntan a servicios de terceros que pueden ser reclamados por cualquiera.
						<ul>
							<li style="font-size: 0.7em;">Activos olvidados de la empresa</li>
							<li style="font-size: 0.7em;">Principalmente subdominios ocultos en niveles profundos</li>
							<li style="font-size: 0.7em;">Impacto en la reputación de la marca y en los datos de los usuarios</li>
							<li style="font-size: 0.7em;">La mayoría de las veces requiere una suscripción de pago</li>
							<li style="font-size: 0.7em;"><a href="https://github.com/projectdiscovery/nuclei-templates/tree/main/takeovers">Nuclei templates FTW</a></li>
							<li style="font-size: 0.7em;">Fix: Cuida tus registros DNS, limpiezas periódicas</li>
						</ul>
					</section>
				</section>
				<section>
					<h2>Sacando las armas</h2>
					<section>
						<p>
							<img src="https://media.giphy.com/media/e2Qlt1gyZmFAbOhly4/giphy.gif" width="auto" height="auto"/>
						</p>
					</section>
					<section>
						<h4>
							Fuerza bruta en login/creds por defecto
						</h4>
						<p>Nuclei tienen una gran biblioteca de plantillas con credenciales predeterminadas en función de la tecnología en configuraciones por defecto.
						Las plantillas de nuclei son bastante sencillas de crear por tu cuenta en caso de que quieras 
						probar una gran cantidad de instancias de una aplicación específica.</p>

						<p>Otras opciones incluyen tener a mano una lista de crendenciales y ejecutarla con ffuf. Esto requiere conocer un poco más
						el contexto de la aplicación (igual que para crear una plantilla de Nuclei) y no es fácil automatizar globalmente.</p>
					</section>
					<section>
						<h3>
							Recolectores de urls: Crawler
						</h3>
						<h4>Katana</h4>
						<p style="font-size: 0.7em;">Un crawler que devuelve los enlaces encontrados en el sitio objetivo. Tiene una gran cantidad de opciones de configuración
						para no salirse de scope, así como incluir cookies de autenticación o cualquier tipo de headers. Además tiene un modo "headless"
						que permite interpretar el DOM o JS dinámicos complejo como si se tratase de un navegador web más.
						</p>
						<pre><code>katana -list url_list.txt -jc -kf -cs target.tld -ef jpg,png</code></pre>
					</section>
					<section>
						<br>
						<br>
						<h3>
							Recolectores de urls: Pasivo / histórico
						</h3>
						<h4>gau / waymore / github-endpoints</h4>
						<p style="font-size: 0.7em;">Extraer urls de un dominio desde diferentes servicios de terceros, como WaybackMachine, VirusTotal o URLScan. A veces es más útil que los crawlers/spiders porque pueden contener parámetros GET y darnos una idea de cómo se usan los endpoints, incluso pueden revelar información sensible</p>
						<pre><code>		# gau
		cat sites.txt | gau
		# waymore
		python3 waymore.py -i webs.txt -mode U -f -O file.txt
		# github-endpoints
		github-endpoints -d target.tld -t key.txt -o output.txt</code></pre>
					</section>
					<section>
						<h4>
							Análisis URLs (<a href="https://github.com/1ndianl33t/Gf-Patterns">patrones GF</a>)
						</h4>
						<p>Wrapper de grep escrito en Go para la clasificación de urls y la búsqueda de patrones potencialmente vulnerables</p>
						<ul>
							<li style="font-size: 0.7em;">SSRF</li>
							<li style="font-size: 0.7em;">SQLi</li>
							<li style="font-size: 0.7em;">base64</li>
							<li style="font-size: 0.7em;">Clasificación por extensiones</li>
						</ul>
					</section>
					<section>
						<h4>
							Wordlists contextuales: basadas en el target
						</h4>
						<p>Se pueden utilizar herramientas de TomNomNom como <a href="https://github.com/tomnomnom/unfurl">unfurl</a>
							para extraer partes interesantes de las urls y crear la base de una wordlist personalizada. Normalmente, estas partes interesantes incluyen parámetros, valores, rutas, etc.</p>

						<p>Existen algunas herramientas antiguas y fiables como <a href="https://github.com/digininja/CeWL">CeWL</a> para la generación de listas de palabras basadas en objetivos a partir 
						de todo el contenido del sitio.</p>
					</section>
					<section>
						<h4>
							Wordlists contextuales: basadas en la tecnología
						</h4>
						<p>OneListForAll tiene una buena colección de <a href="https://github.com/six2dez/OneListForAll/tree/main/dict">wordlists separadas por tecnologías</a>.</p>
						<p>Assetnote <a href="https://wordlists.assetnote.io/">tiene una colección</a> de wordlists específicamente elaboradas y basadas en el contexto.</p>
					</section>
					<section>
						<h4>
							Escáneres de vulnerabilidades específicos
						</h4>
						<p>Herramientas creadas en torno a una tecnología específica centradas en diferentes vulnerabilidades concretas</p>
						<ul>
							<li style="font-size: 0.7em;">WPScan</li>
							<li style="font-size: 0.7em;">CMSeek</li>
							<li style="font-size: 0.7em;">aem-hacker</li>
							<li style="font-size: 0.7em;">sret</li>
						</ul>
					</section>
					<section>
						<h4>
							Análisis JS
						</h4>
						<p>Todavía hay mucho margen de mejora en este aspecto, ya que el análisis sintáctico y la renderización de JS son difíciles de automatizar</p>
						<ul>
							<li style="font-size: 0.7em;"><a href="https://github.com/w9w/JSA">Búsqueda</a> de JS <a href="https://github.com/xnl-h4ck3r/xnLinkFinder">recursiva</a></li>
							<li style="font-size: 0.7em;">Recolectores de endpoints</li>
							<li style="font-size: 0.7em;"><a href="https://github.com/projectdiscovery/nuclei-templates/tree/main/exposures">Secretos en JS</a></li>
						</ul>
					</section>
					<section>
						<h3>Responders: interactsh</h3>
						<p>Está disponible como solución hosteada por PDiscovery, pero se le saca mucho más partido si se hostea uno propio. 
						Además cuando se trata de posibles vulnerabilidades, es importante que tú seas el único propietario de los datos</p>
						<p><a href="https://github.com/projectdiscovery/interactsh">https://github.com/projectdiscovery/interactsh</a></p>
						<p>Para hostear el server se necesita un servidor (VPS) con una dirección IP estática y un nombre de dominio.</p>
						<p>En el contexto de pentesting web, se utiliza a menudo sólo como callback DNS para capturar peticiones SSRF.</p>
					</section>
					<section>
						<h3>Responders: XSSHunter</h3>
						<p>Esta es una gran herramienta para gestionar eficientemente vulnerabilidades XSS, especialmente para blind XSS. El XSSHunter original
						fue descontinuado hace poco y ha sido sustituido por un fork ligeramente modificado y hosteada por Trufflesec.</p>
						<p>Para subrayar aún más que "debes hostearlo por ti mismo" (tanto la instancia como la propiedad de los datos), recientemente ha habido controversia
						con Trufflesec por farmear y publicar información de la base de datos de su version hosteada, supuestamente para estadísticas de uso.</p>
						<p>OG: <a href="https://github.com/mandatoryprogrammer/xsshunter-express">https://github.com/mandatoryprogrammer/xsshunter-express</a></p>
						<p>Trufflesec fork: <a href="https://github.com/trufflesecurity/xsshunter">https://github.com/trufflesecurity/xsshunter</a></p>
					</section>
					<section>
						<h4>
							Web fuzzing avanzado
						</h4>
						<p>Ffuf es una gran herramienta para este propósito. Está diseñado para ser muy versátil, rollo navaja suiza para web fuzzing.</p>
						<img src="ffuf_logo.png" style="width: 40%;margin: -30px;">
						<p>A continuación se mostrarán algunos ejemplos de escaneos automatizados que se pueden hacer con ffuf, pero se puede hacer mucho más.
							Para conocer más a fondo las distintas opciones disponibles, pásate por la <a href="https://github.com/ffuf/ffuf/wiki">documentación de ffuf</a>.</p>

					</section>
					<section>
						<h4>Advanced web fuzzing: Extracción de credenciales</h4>
						<p>Ffuf proporciona una manera de añadir <a href="https://github.com/ffuf/ffuf/wiki/Scraper">scraper rules</a>.Son archivos json que se pueden guardar en $HOME/.config/ffuf/scraper/</p>
						<p>Un archivo de ejemplo que puede utilizarse en el taller está en el <a href="https://github.com/hconrecon23/workshop/blob/master/aws_creds.json">repo</a>. Éstas reglas se aplican pasivamente a todos los escaneos de ffuf si guardas el archivo en el directorio global de scrapers. Para un solo uso, se puede hacer referencia a un archivo de scraper con el parámetro -scraperfile.</p>
						<pre data-id="code-animation"><code># -mc 999 deshabilita la salida hasta que el scraper salte
ffuf -w all_urls.txt -u FUZZ -mc 999 \
	   -scraperfile aws_creds.json</code></pre>
					</section>
					<section>
						<h4>
							Advanced web fuzzing: Virtualhosts
						</h4>
						<p>Se pruebam todos los nombres de host contra todos los targets web y devuelve todos los vhosts que reciben respuesta de
						de CDNs/load balancers. Esto puede llevar a veces a divertidas fallos de configuración.</p>
						<pre data-id="code-animation"><code>ffuf -w hostnames.txt:VHOST -w hostnames.txt:DOMAIN \
				-u https://DOMAIN -H "Host: VHOST" -ac -ach
						</code></pre>
					</section>
					<section>
						<br>
						<h4>
							Advanced web fuzzing: Headers (blind)
						</h4>
						<p>Se combina una lista de headers junto con una lista de payloads para encontrar diferentes vulnerabilidades.
						Puedes ir a lo loco e incluir todo en la misma petición pero sería más complicado encontrar el payload bueno.</p>
						<p>Normalmente esto lleva a XSS (a través de paneles administrativos normalmente), proxies inversos mal configurados, balanceadores de carga, etc.</p>
						<p>Es interesante lanzar una ejecución por cada tipo de vuln. Por ejemplo:
						filtrado basado en el tiempo para time-based SQLi</p>
						<pre data-id="code-animation"><code>ffuf -w headernames.txt:HEADERNAME -w payloads.txt:PAYLOAD \
				-w hostnames.txt:TARGET \
				-u https://TARGET \
				-H "HEADERNAME: PAYLOAD"</code></pre>
					</section>
					<section>
						<h4>
							Advanced web fuzzing: SSTI
						</h4>
						<p>Puedes crear tu propia wordlist de payloads SSTI fáciles de identificar. Un buen recurso para empezar sería la página de 
						<a href="https://github.com/swisskyrepo/PayloadsAllTheThings/tree/master/Server%20Side%20Template%20Injection">PayloadsAllTheThings</a>
							sobre SSTI.</p>
						<p>Asegúrate de que el cálculo te devuelve algo único y fácil de contrastar. El típico {{7*7}} puede llevar a un montón
						de falsos positivos ;)</p>
						<pre data-id="code-animation"><code>ffuf -w ssti_payloads.txt:SSTI -w get_params.txt:GETPARAM \
				-w hostnames.txt:TARGET \
				-u https://TARGET/?GETPARAM=SSTI \
				-mr 'result_of_your_unique_calculation_here'</code></pre>
					</section>
					<section>
						<h4>
							Advanced web fuzzing: LFI
						</h4>
						<p>Puedes seguir el mismo principio que con los SSTI. En este caso interesa lanzar una ejecución de ffuf por cada payload
						en caso de que estés intentando leer diferentes archivos del filesystem.</p>
						<p>Puedes usar la 
							<a href="https://github.com/danielmiessler/SecLists/blob/master/Fuzzing/LFI/LFI-Jhaddix.txt">wordlist de LFI </a>
						de Jhaddix, incluída en SecLists.</p>
						<p>En el siguiente ejemplo se usa la wordlist de LFI con diferentes técnicas para leer /etc/passwd</p>
						<pre data-id="code-animation"><code>ffuf -w lfi_payloads.txt:LFI -w get_params.txt:GETPARAM \
				-w hostnames.txt:TARGET \
				-u https://TARGET/?GETPARAM=LFI \
				-mr 'root:x:0:0:root'</code></pre>
					</section>
					<section>
						<h4>
							Blind SSRF payloads
						</h4>
						<p>ffuf 2.0 tiene una función de mapeo inverso llamada <b>FFUFHASH</b> que permite añadir un placeholder único
						para payloads de blind SSRF. Esto funciona especialmente bien con interactsh. En el siguiente ejemplo el
						dominio de interactsh sería <b>iash.evil.com</b>:</p>
						<pre data-id="code-animation"><code>ffuf -w params.txt -u \
		https://target.tld/?FUZZ=%2F%2FFFUFHASH.iash.evil.com</code></pre>
					</section>
				</section>
				<section>
					<h2>Escalando</h2>
					<section>
						<img src="https://media.giphy.com/media/6bh9FTTEKdtZe/giphy.gif" />
					</section>
					<section>
						<img src="axiom_banner.png" style="width: 50%;"/>
						<ul>
                            <li>La herramienta divide las wordlists y otros métodos de entrada entre las instancias de la flota</li>
							<li>Los resultados se recuperan y se vuelven a unir en la máquina controladora</li>
							<li><a href="https://github.com/pry0cc/axiom">https://github.com/pry0cc/axiom</a></li>
                        </ul>
					</section>
					<section>
						<h4>Axiom</h4>
						<pre><code>
							# Building the image
							axiom-build
							
							# Spin up the fleet
							axiom-fleet "1337squadron" -i 50
							
							# Scanning
							axiom-scan input.txt -m ffuf -o output.txt 

							# Deleting the fleet
							axiom-rm -f "\*"
						</code></pre>
						<p style="font-size: 0.7em;">⚠️ Alto riesgo de DDOS</p>
					</section>
					<section>
						<img src="serverless.jpeg" style="width: 30%"/>
						<ul>
                            <li>Escala infinitamente, pero puede ser caro</li>
							<li>Levanta miles de funciones AWS Lambda para ejecutar la tarea de forma extremadamente rápida</li>
							<li><a href="https://github.com/fyoorer/ShadowClone">https://github.com/fyoorer/ShadowClone</a></li>
                        </ul>
					</section>
					<section>
						<br>
						<h4>Shadowclone</h4>
						<ul>
                            <li>Config: <a href="https://github.com/fyoorer/ShadowClone/wiki">https://github.com/fyoorer/ShadowClone/wiki</a></li>
							<li>Nota: las políticas de AWS obligan incluir también el rol "iam:PassRole"</li>
							<li>La wiki está un poco desactualizada pero los siguientes comandos deberían funcionar:</li>
                        </ul>
						<pre><code># build your container
lithops runtime build sc-runtime -f Dockerfile

# deploy it with 512Mb memory and 300sec timeout
lithops runtime deploy --memory 512 --timeout 300 sc-runtime</code></pre>
						<ul>
                            <li style="font-size: 0.7em;">1m 40s para escanear 700k websites con una sola plantilla</a></li>
							<li style="font-size: 0.7em;">10m para mapear las tecnologías de 700k websites</a></li>
						</ul>
					</section>
				</section>
				<section>
					<h2>Automatización</h2>
					<section>
						<h4>Uniendo todas las piezas</h4>
						<img src="automation.gif" />
					</section>
					<section>
                        <h4>Scripting</h4>
                        <p style="font-size: 0.7em;">Para permitir una fácil extensión y construcción iterativa de la automatización lo mejor es utilizar Python o shell scripting.</p>
                        <ul>
                            <li style="font-size: 0.7em;">Beneficios de 🐍
                                <ul>
                                    <li>Capacidad de actuar inmediatamente en función de los resultados, basado en "eventos"</li>
                                    <li>Facilidad para filtrar los resultados dentro del flujo</li>
                                    <li>Más control sin amplios conocimientos profundos del lenguaje</li>
                                    <li>Control de trabajos (potencialmente) paralelos</li>
                                </ul>
                            </li>
                            <li style="font-size: 0.7em;">Beneficios de 🐚
                                <ul>
                                    <li>Se empieza extremadamente rápido</li>
                                    <li>Muchas herramientas de software libre a tu disposición</li>
                                    <li>Fácil de ampliar aún más con frameworks como Axiom</li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h4>Parseo</h4>
                        <p style="font-size: 0.7em;">JSON está hecho para ser leído por ordenadores y es tu mejor aliado. La mayoría de las herramientas lo soportan de una forma u otra. En general, estarás tratando con archivos de texto o con JSON.</p>
                        <ul>
                            <li>Python soporta JSON nativamente</li>
                            <li><a href="https://github.com/stedolan/jq">jq</a> es una herramienta CLI impresionante para parsear y filtrar JSON</li>
                            <li>Se puede grepear JSON con <a href="https://github.com/tomnomnom/gron">gron</a></li>
                            <li>O seleccionar partes específicas de las URL con <a href="https://github.com/tomnomnom/unfurl">unfurl</a></li>
                        </ul>
                    </section>
                    <section>
                        <h4>Piping</h4>
                        <p style="font-size: 0.7em;">Muchas etapas diferentes en el proceso de automatización pueden bifurcarse en múltiples subtareas, por lo que a menudo es bueno almacenar los resultados de las etapas anteriores en un archivo temporal. Esto también suele ayudar a depurar posibles problemas.</p>
                        <ul>
                            <li>Usa <a href="https://man7.org/linux/man-pages/man1/tee.1.html">tee</a> para añadir stdout a un archivo mientras también se muestra por terminal</li>
                            <li><a href="https://github.com/tomnomnom/anew">anew</a> solo añade las nuevas líneas al archivo de salida</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Filtrado</h4>
                        <p style="font-size: 0.7em;">Trata de evitar la creación de tráfico innecesario, y sobre todo salirte fuera del scope, lo que puede suceder con bastante facilidad. Asegúrate de filtrar las entradas que no quieres de tu workflow desde el principio. Esto hará que tu automatización sea más rápida, y causará considerablemente menos problemas legales en el futuro ;)</p>
                        <p style="font-size: 0.7em;">Puedes encontrar un ejemplo de un sencillo script en Python que lee líneas de stdin (pensadas para ser utilizadas dentro en un pipe) y devuelve sólo las urls http(s) dentro del scope <a href="https://github.com/hconrecon23/workshop/blob/master/inscope_http.py">aquí</a></p>
                    </section>
					<section>
                        <h4>Frameworks</h4>
                        <p style="font-size: 0.7em;">Conjunto de herramientas y técnicas cuyo objetivo es automatizar todo el proceso de reconocimiento, proporcionando un flujo de trabajo personalizable (o no) para lograr el objetivo.</p>
						<ul>
                            <li><a href="https://github.com/smicallef/spiderfoot">reNgine</a>: web ui, Python, Docker, Postgres DB, yaml workflows, workflows predefinados + posibilidad de custom</li>
							<li><a href="https://github.com/epi052/recon-pipeline">recon-pipeline</a>: cli, Python, sqlite DB, listado de herramientas cerradas, escáneres predefinidos y posibilicad de customs</li>
                            <li><a href="https://github.com/six2dez/reconftw">reconFTW</a>: cli sin UI (<a href="https://www.youtube.com/watch?v=rsIj7bFx4dk">de momento</a>), archivos de texto, listado cerrado de herramientas y workflow, customizable, Docker, Terraform, integración con axiom</li>
                        </ul>
                    </section>
				</section>
				<section>
					<h2>Aplicación práctica</h2>
					<section>
						<img src="https://media.giphy.com/media/3oKIPwoeGErMmaI43S/giphy.gif" width="50%" height="auto"/>
					</section>
					<section>
                        <h4>Solución personalizada de mapeo de superficies de ataque</h4>
                    </section>
					<section>
                        <h4>Proceso previo al comienzo del pentest</h4>
                    </section>
					<section>
                        <h4>Bug bounty</h4>
                    </section>
				</section>
			</div>
		</div>
		<script src="dist/reveal.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/search/search.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>

			// Also available as an ES module, see:
			// https://revealjs.com/initialization/
			Reveal.initialize({
				controls: true,
				progress: true,
				center: true,
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight ]
			});

		</script>

	</body>
</html>
